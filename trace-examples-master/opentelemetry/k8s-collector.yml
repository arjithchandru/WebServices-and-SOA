---
# Give admin rights to the default account
# so that k8s_tagger can fetch info
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fabric8-rbac
subjects:
  - kind: ServiceAccount
    name: default
    namespace: default
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-agent-conf
  labels:
    app: opentelemetry
    component: otel-agent-conf
data:
  otel-agent-config: |
    receivers:
      otlp:
        protocols:
          grpc:
          http:
      jaeger:
        protocols:
          thrift_http:
      zipkin:
    exporters:
      otlp:
        endpoint: "otel-collector.default:55680"
        insecure: true
      logging:
    processors:
      batch:
      # The resource detector injects the pod IP
      # to every metric so that the k8s_tagger can
      # fetch information afterwards.
      resourcedetection:
        detectors: [env]
        timeout: 5s
        override: false
      # The k8s_tagger in the Agent is in passthrough mode
      # so that it only tags with the minimal info for the
      # collector k8s_tagger to complete
      k8s_tagger:
        passthrough: true      
    extensions:
      health_check: {}
    service:
      extensions: [health_check]
      pipelines:
        traces:
          receivers: [otlp, zipkin, jaeger]
          # resourcedetection must come before k8s_tagger
          processors: [batch, resourcedetection, k8s_tagger]
          # processors: [batch]
          exporters: [otlp, logging]
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: otel-agent
  labels:
    app: opentelemetry
    component: otel-agent
spec:
  selector:
    matchLabels:
      app: opentelemetry
      component: otel-agent
  template:
    metadata:
      labels:
        app: opentelemetry
        component: otel-agent
    spec:
      containers:
      - command:
          - "/otelcontribcol"
          - "--config=/conf/otel-agent-config.yaml"
          # Memory Ballast size should be max 1/3 to 1/2 of memory.
          - "--mem-ballast-size-mib=165"
          - "--log-level=debug"
        image: otel/opentelemetry-collector-contrib:latest
        imagePullPolicy: IfNotPresent
        name: otel-agent
        resources:
          limits:
            cpu: 500m
            memory: 500Mi
          requests:
            cpu: 100m
            memory: 400Mi
        env:
           # Get pod ip so that k8s_tagger can tag resources
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
            # We set the cluster name manually so that we can test all behavior
            # This is picked up by the resource detector
          - name: OTEL_RESOURCE
            value: "k8s.pod.ip=$(POD_IP)"
        ports:
        - containerPort: 6831 # Jaeger Thrift Compact
          protocol: UDP
        - containerPort: 8888 # Prometheus Metrics
        - containerPort: 9411 # Default endpoint for Zipkin receiver.
        - containerPort: 14250 # Default endpoint for Jaeger gRPC receiver.
        - containerPort: 55681 # Default OpenTelemetry HTTP receiver port.
          hostPort: 55681
          name: traceporthttp
          protocol: TCP        
        - containerPort: 55680
          hostPort: 55680
          name: traceportgrpc
          protocol: TCP
        - containerPort: 14268 # Default endpoint for Jaeger HTTP receiver.
          hostPort: 14268
          name: jaegerporthttp
          protocol: TCP 
        volumeMounts:
        - name: otel-agent-config-vol
          mountPath: /conf
        livenessProbe:
          httpGet:
            path: /
            port: 13133 # Health Check extension default port.
        readinessProbe:
          httpGet:
            path: /
            port: 13133 # Health Check extension default port.
      volumes:
        - configMap:
            name: otel-agent-conf
            items:
              - key: otel-agent-config
                path: otel-agent-config.yaml
          name: otel-agent-config-vol
---
# Define a daemonset for fluentbit
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit
  labels:
    app: otel
    component: fluent-bit
spec:
  selector:
    matchLabels:
      app: otel
      component: fluent-bit
  template:
    metadata: 
      labels:
        app: otel
        component: fluent-bit
        kubernetes.io/cluster-service: "true"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "2020"
        prometheus.io/path: /api/v1/metrics/prometheus
    spec:
      containers:
      - name: fluent-bit
        image: fluent/fluent-bit:1.5
        imagePullPolicy: Always
        ports:
          - containerPort: 2020
        env:
        - name: FLUENT_DATADOG_HOST
          value: "http-intake.logs.datadoghq.com"
        - name: FLUENT_DATADOG_API_KEY
          value: #"<YOUR_API_KEY>"
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: fluent-bit-config
          mountPath: /fluent-bit/etc/
        - name: mnt
          mountPath: /mnt
          readOnly: true
      terminationGracePeriodSeconds: 10
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: fluent-bit-config
        configMap:
          name: fluent-bit-config
      - name: mnt
        hostPath:
          path: /mnt
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  labels:
    app: otel
    component: fluent-bit
data:
  # Configuration files: server, input, filters and output
  # ======================================================
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf
        HTTP_Server   On
        HTTP_Listen   0.0.0.0
        HTTP_Port     2020

    @INCLUDE input-kubernetes.conf
    @INCLUDE filter-kubernetes.conf
    @INCLUDE output-datadog.conf

  input-kubernetes.conf: |
    [INPUT]
        Name              tail
        Tag               kube.*
        Path              /var/log/containers/*.log
        Parser            docker
        DB                /var/log/flb_kube.db
        Mem_Buf_Limit     5MB
        Skip_Long_Lines   On
        Refresh_Interval  10

  filter-kubernetes.conf: |
    [FILTER]
        Name                kubernetes
        Match               kube.*
        Kube_URL            https://kubernetes.default.svc:443
        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
        Kube_Tag_Prefix     kube.var.log.containers.
        Merge_Log           On
        Merge_Log_Key       log_processed
        K8S-Logging.Parser  On
        K8S-Logging.Exclude Off

  output-datadog.conf: |
    [OUTPUT]
        Name              datadog
        Match             *
        Host              ${FLUENT_DATADOG_HOST}
        TLS               off
        compress          gzip
        apikey            ${FLUENT_DATADOG_API_KEY}
        dd_source         otel
        dd_message_key    log_processed

  parsers.conf: |
    [PARSER]
        Name   apache
        Format regex
        Regex  ^(?<host>[^ ]*) [^ ]* (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*?)(?: +\S*)?)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$
        Time_Key time
        Time_Format %d/%b/%Y:%H:%M:%S %z

    [PARSER]
        Name   apache2
        Format regex
        Regex  ^(?<host>[^ ]*) [^ ]* (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^ ]*) +\S*)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$
        Time_Key time
        Time_Format %d/%b/%Y:%H:%M:%S %z

    [PARSER]
        Name   apache_error
        Format regex
        Regex  ^\[[^ ]* (?<time>[^\]]*)\] \[(?<level>[^\]]*)\](?: \[pid (?<pid>[^\]]*)\])?( \[client (?<client>[^\]]*)\])? (?<message>.*)$

    [PARSER]
        Name   nginx
        Format regex
        Regex ^(?<remote>[^ ]*) (?<host>[^ ]*) (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*?)(?: +\S*)?)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$
        Time_Key time
        Time_Format %d/%b/%Y:%H:%M:%S %z

    [PARSER]
        Name   json
        Format json
        Time_Key time
        Time_Format %d/%b/%Y:%H:%M:%S %z

    [PARSER]
        Name        docker
        Format      json
        Time_Key    time
        Time_Format %Y-%m-%dT%H:%M:%S.%L
        Time_Keep   On

    [PARSER]
        # http://rubular.com/r/tjUt3Awgg4
        Name cri
        Format regex
        Regex ^(?<time>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<message>.*)$
        Time_Key    time
        Time_Format %Y-%m-%dT%H:%M:%S.%L%z

    [PARSER]
        Name        syslog
        Format      regex
        Regex       ^\<(?<pri>[0-9]+)\>(?<time>[^ ]* {1,2}[^ ]* [^ ]*) (?<host>[^ ]*) (?<ident>[a-zA-Z0-9_\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?(?:[^\:]*\:)? *(?<message>.*)$
        Time_Key    time
        Time_Format %b %d %H:%M:%S
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-conf
  labels:
    app: opentelemetry
    component: otel-collector-conf
data:
  otel-collector-config: |
    receivers:
      otlp:
        protocols:
          grpc:
          http:
    processors:
      batch:
        timeout: 10s
      k8s_tagger:
    extensions:
      health_check: {}
      zpages: {}
    exporters:
      datadog:
        api:
          key: <YOUR_API_KEY>
      logging:
    service:
      extensions: [health_check, zpages]
      pipelines:
        metrics/2:
          receivers: [otlp]
          processors: [batch, k8s_tagger]
          
          exporters: [datadog]
        traces/2:
          receivers: [otlp]
          processors: [batch, k8s_tagger]
          exporters: [datadog,logging]
---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  labels:
    app: opentelemetry
    component: otel-collector
spec:
  ports:
  - name: otlp # Default endpoint for OpenTelemetry receiver.
    port: 55680
    protocol: TCP
    targetPort: 55680
  - name: metrics # Default endpoint for querying metrics.
    port: 8888
  selector:
    component: otel-collector
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  labels:
    app: opentelemetry
    component: otel-collector
spec:
  selector:
    matchLabels:
      app: opentelemetry
      component: otel-collector
  minReadySeconds: 5
  progressDeadlineSeconds: 120
  replicas: 1
  template:
    metadata:
      labels:
        app: opentelemetry
        component: otel-collector
    spec:
      containers:
      - command:
          - "/otelcontribcol"
          - "--config=/conf/otel-collector-config.yaml"
          - "--log-level=debug"
        image: otel/opentelemetry-collector-contrib:latest
        imagePullPolicy: IfNotPresent
        name: otel-collector
        resources:
          limits:
            cpu: 1
            memory: 2Gi
          requests:
            cpu: 200m
            memory: 400Mi
        ports:
        - containerPort: 55679 # Default endpoint for ZPages.
        - containerPort: 55680 # Default endpoint for OpenTelemetry receiver.
        - containerPort: 8888  # Default endpoint for querying metrics.
        volumeMounts:
        - name: otel-collector-config-vol
          mountPath: /conf
        livenessProbe:
          httpGet:
            path: /
            port: 13133 # Health Check extension default port.
        readinessProbe:
          httpGet:
            path: /
            port: 13133 # Health Check extension default port.
      volumes:
        - configMap:
            name: otel-collector-conf
            items:
              - key: otel-collector-config
                path: otel-collector-config.yaml
          name: otel-collector-config-vol
---
# Define a service for the python-microservice
apiVersion: v1
kind: Service
metadata:
  name: python-microservice
  labels:
    app: opentelemetry
    component: python-microservice
spec:
  ports:
  - name: http
    port: 5000
    protocol: TCP
    targetPort: 5000
    nodePort: 31367
  selector:
    app: opentelemetry
    component: python-microservice
  type: NodePort
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: python-microservice
  labels:
    app: opentelemetry
    component: python-microservice
spec:
  selector:
    matchLabels:
      app: opentelemetry
      component: python-microservice
  minReadySeconds: 5
  progressDeadlineSeconds: 120
  replicas: 1
  template:
    metadata:
      labels:
        app: opentelemetry
        component: python-microservice
    spec:
      containers:
      - image: python-microservice  
        imagePullPolicy: IfNotPresent
        name: python-microservice
        resources:
          limits:
            cpu: 1
            memory: 600Mi
          requests:
            cpu: 200m
            memory: 400Mi
        ports:
        - containerPort: 5000
          protocol: TCP
        env:        
        - name: HOST_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        - name: OTEL_EXPORTER_OTLP_ENDPOINT
          value: "$(HOST_IP):55680"
        - name: OTEL_RESOURCE_ATTRIBUTES
          value: deployment.environment=otel_sandbox,version=v1
---
# Define a service for the node-microservice
apiVersion: v1
kind: Service
metadata:
  name: node-microservice
  labels:
    app: opentelemetry
    component: node-microservice
spec:
  ports:
  - name: http
    port: 4000
    protocol: TCP
    targetPort: 4000
  selector:
    app: opentelemetry
    component: node-microservice
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-microservice
  labels:
    app: opentelemetry
    component: node-microservice
spec:
  selector:
    matchLabels:
      app: opentelemetry
      component: node-microservice
  minReadySeconds: 5
  progressDeadlineSeconds: 120
  replicas: 1
  template:
    metadata:
      labels:
        app: opentelemetry
        component: node-microservice
    spec:
      containers:
      - image: node-microservice:latest
        imagePullPolicy: IfNotPresent
        name: node-microservice
        resources:
          limits:
            cpu: 1
            memory: 600Mi
          requests:
            cpu: 200m
            memory: 400Mi
        ports:
        - containerPort: 4000
          protocol: TCP
        env:        
        - name: HOST_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        # opentelemetry-js only has json over http at this time 
        - name: OTEL_EXPORTER_OTLP_HTTP_ENDPOINT
          value: "$(HOST_IP):55681"
        - name: OTEL_RESOURCE_ATTRIBUTES
          value: deployment.environment=otel_sandbox,version=v1
---
# Define a service for the ruby-microservice
apiVersion: v1
kind: Service
metadata:
  name: ruby-microservice
  labels:
    app: opentelemetry
    component: ruby-microservice
spec:
  ports:
  - name: http
    port: 3000
    protocol: TCP
    targetPort: 3000
  selector:
    app: opentelemetry
    component: ruby-microservice
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ruby-microservice
  labels:
    app: opentelemetry
    component: ruby-microservice
spec:
  selector:
    matchLabels:
      app: opentelemetry
      component: ruby-microservice
  minReadySeconds: 5
  progressDeadlineSeconds: 120
  replicas: 1
  template:
    metadata:
      labels:
        app: opentelemetry
        component: ruby-microservice
    spec:
      containers:
      - image: ruby-microservice:latest
        imagePullPolicy: IfNotPresent
        name: ruby-microservice
        resources:
          limits:
            cpu: 1
            memory: 600Mi
          requests:
            cpu: 200m
            memory: 400Mi
        ports:
        - containerPort: 3000
          protocol: TCP
        env:
        - name: HOST_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        # was having trouble with the otlp export but jaeger also exists in core by default
        - name: OTEL_EXPORTER_JAEGER_ENDPOINT
          value: "http://$(HOST_IP):14268/api/traces"
        - name: OTEL_RESOURCE_ATTRIBUTES
          value: deployment.environment=otel_sandbox,version=v1
        - name: OTEL_TRACES_EXPORTER
          value: jaeger
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vegeta
  labels:
    app: opentelemetry
    component: vegeta
spec:
  selector:
    matchLabels:
      app: opentelemetry
      component: vegeta
  minReadySeconds: 5
  progressDeadlineSeconds: 120
  replicas: 1
  template:
    metadata:
      labels:
        app: opentelemetry
        component: vegeta
    spec:
      containers:
      - name: client
        imagePullPolicy: Always
        image: peterevans/vegeta:6.8.1
        command:
          - sh
          - '-c'
          - "sleep 5 && echo GET http://python-microservice:5000/ | vegeta attack -duration=10m -rate=1"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongo
  labels:
    app: opentelemetry
    component: mongo
spec:
  selector:
    matchLabels:
      app: opentelemetry
      component: mongo
  minReadySeconds: 5
  progressDeadlineSeconds: 120
  replicas: 1
  template:
    metadata:
      labels:
        app: opentelemetry
        component: mongo
    spec:
      containers:
      - name: mongo
        imagePullPolicy: Always
        image: mongo:3.5
        ports:
        - containerPort: 27017
          protocol: TCP
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  labels:
    app: opentelemetry
    component: redis
spec:
  selector:
    matchLabels:
      app: opentelemetry
      component: redis
  minReadySeconds: 5
  progressDeadlineSeconds: 120
  replicas: 1
  template:
    metadata:
      labels:
        app: opentelemetry
        component: redis
    spec:
      containers:
      - name: redis
        imagePullPolicy: Always
        image: redis
        ports:
        - containerPort: 6379
          protocol: TCP
---
# Define a service for mongo
apiVersion: v1
kind: Service
metadata:
  name: mongo
  labels:
    app: opentelemetry
    component: mongo
spec:
  ports:
  - name: http
    port: 27017
    protocol: TCP
    targetPort: 27017
  selector:
    app: opentelemetry
    component: mongo
---
# Define a service for redis
apiVersion: v1
kind: Service
metadata:
  name: redis
  labels:
    app: opentelemetry
    component: redis
spec:
  ports:
  - name: http
    port: 6379
    protocol: TCP
    targetPort: 6379
  selector:
    app: opentelemetry
    component: redis
---
